def hindi_correction_prompt(chunk):
    return f"""
You are a Hindi language expert with a deep understanding of transcription, conversational nuances, and respectful language handling.

You have been given a raw transcription of a Hindi conversation. This text may include:
- Spelling or typographical errors (e.g., "आमाद" instead of "आमद")
- Words that are joined together without spaces (e.g., "मैंनानसी")
- Words that ar
e incorrectly split (e.g., "अ स्थि" instead of "अस्थि")
- Punctuation and formatting inconsistencies
- Instances of English words written in Devanagari script due to transliteration (e.g., "ड्राफ्ट", "क्लिनिक", "ट्रांसप्लांट")

Your task is to:
- Gently correct spelling, spacing, and punctuation, ensuring the readability and natural flow of the conversation
- Thoughtfully identify transliterated English words written in Devanagari script and replace them with their correct English spelling (e.g., "क्लिनिक" → "clinic"), only where appropriate and commonly understood
- Retain the overall structure, intent, and cultural tone of the conversation
- Maintain the speaker-label format as it appears (e.g.,  
  Speaker 1: ...  
  Speaker 2: ...)

Do **not** translate the conversation into English.  
Do **not** alter or rename the speaker labels.  
Be respectful and mindful of cultural context—avoid overcorrecting informal or colloquial language unless it disrupts clarity.

Raw Conversation:
\"\"\"{chunk}\"\"\"

Cleaned and culturally sensitive version (with thoughtful handling of transliterated terms):
"""

def translation_prompt(chunk):
    return f"""
You are a professional Hindi-English translator.

Translate the following Hindi (Devanagari) text into natural, fluent English while preserving the tone and speaker formatting.

Hindi Text:
\"\"\"{chunk}\"\"\"

English Translation:
"""

def sales_call_analysis_prompt(transcript):
    return f"""
You are an expert in analyzing sales call transcripts.

You are provided with a transcript of a conversation between a QHT Clinic salesperson and a prospective client. The speakers are not labeled. You must:

---

**Task 1**: Infer the speakers based on the content and context.

**Task 2**: Critically evaluate the salesperson's performance using the following JSON structure. All feedback must be highly specific, contextually relevant, and constructive.

---

**Instructions**: Populate each key with your analysis. Return only the resulting JSON object with no additional explanation.

Return your analysis in this exact JSON format:

{{
  "Pitch Followed": {{
    1. "Detailed Analysis": "Explain how well the salesperson followed a structured pitch, demonstrated knowledge of QHT's procedure, and conveyed benefits clearly."
    2. "Positive Example": "Quote one effective line from the salesperson and explain why it was good."
    3. "Negative Example": "Quote one ineffective line and explain why it was weak."
    4. "Suggested Sentences": "Provide at least two alternative sentences the salesperson could have said to improve clarity or pitch structure."
  }},
  "Confidence": {{
    1. "Detailed Analysis": "Assess how confidently the salesperson responded to queries, addressed objections, and conveyed authority."
    2. "Positive Example": "Quote and explain one confident line."
    3. "Negative Example": "Quote and explain one hesitant or unclear line."
    4. "Suggested Sentences": "Provide two or more confident, reassuring sentences the salesperson could have used.
  }},
  "Tonality": {{
    1. "Detailed Analysis": "Evaluate the salesperson's tone — empathy, professionalism, warmth, adaptability to customer's mood."
    2. "Positive Example": "Quote and explain a good tone moment."
    3. "Negative Example": "Quote and explain a tone misstep."
    4. "Suggested Sentences": "Suggest better-toned alternatives that would have improved engagement.
  }},
  "Energy": {{
    1. "Detailed Analysis": "Examine the level of enthusiasm shown and whether it was maintained throughout the call."
    2. "Positive Example": "Quote a sentence that reflected strong energy."
    3. "Negative Example": "Quote a sentence that lacked enthusiasm."
    4. "Suggested Sentences": "Suggest phrases that convey excitement and motivation tailored to this transcript."
  }},
  "Objection Handling": {{
    1. "Detailed Analysis": "Analyze how well objections (e.g., cost, pain, recovery) were handled — clarity, empathy, and resolution."
    2. "Positive Example": "Quote an example where an objection was well addressed."
    3. "Negative Example": "Quote a poor handling of an objection."
    4. "Suggested Sentences": "Suggest alternative responses that would better resolve common objections.
  }},
  "Strengths & Areas for Improvement": {{
    1. "Strengths": "Summarize major strengths with specific examples."
    2. "Areas for Improvement": "Highlight weaknesses and suggest what to improve."
  }},
  "Suggested Sentences Recap": {{
    1. "Pitch Followed": []
    2. "Confidence": []
    3. "Tonality": []
    4. "Energy": []
    5. "Objection Handling": []
  }},
  "Scores": {{
    1. "Pitch Followed Score": [Integer 1–10]
    2. "Confidence Score": [Integer 1–10]
    3. "Tonality": [Integer 1–10]
    4. "Energy": [Integer 1–10]
    5. "Objection Handling": [Integer 1–10]
    6. "Overall Score": [Average of the above 5 scores, rounded to 1 decimal]
  }}
}}

Transcript:
\"\"\"{transcript}\"\"\"
"""

import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Get API keys from environment variables
openai_api_key = os.getenv('OPENAI_API_KEY')
elevenlabs_api_key = os.getenv('ELEVENLABS_API_KEY')

if not openai_api_key:
    raise ValueError("OpenAI API key not found in environment variables.")
if not elevenlabs_api_key:
    raise ValueError("ElevenLabs API key not found in environment variables.")

from flask import Flask, request, jsonify
from io import BytesIO
import threading
import uuid
import os
from openai import OpenAI
import requests
import re
from elevenlabs import ElevenLabs

app = Flask(__name__)

# Clients
openai_client = OpenAI(api_key=openai_api_key)
elevenlabs_client = ElevenLabs(api_key=elevenlabs_api_key)


# ----------------------- UTILITY FUNCTIONS -----------------------

def split_by_speaker_sentences(text: str) -> list[str]:
    """Splits transcript into sentences per speaker."""
    blocks = re.split(r'(Speaker\s+\d+:)', text)
    chunks, current_speaker = [], ""
    for part in blocks:
        if re.match(r'Speaker\s+\d+:', part):
            current_speaker = part.strip()
        elif part.strip():
            sentences = re.split(r'(?<=[।!?])\s+', part.strip())
            chunks.extend(f"{current_speaker} {s.strip()}" for s in sentences if s)
    return chunks


def group_chunks_by_size(chunks: list[str], max_chars: int = 1500) -> list[str]:
    """Groups sentence chunks under a character limit."""
    grouped, buffer = [], ""
    for chunk in chunks:
        if len(buffer) + len(chunk) < max_chars:
            buffer += chunk + " "
        else:
            grouped.append(buffer.strip())
            buffer = chunk + " "
    if buffer:
        grouped.append(buffer.strip())
    return grouped


def request_openai(prompt: str, model: str = "gpt-4", temperature: float = 0.3) -> str:
    """Handles OpenAI chat completion requests."""
    response = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature
    )
    return response.choices[0].message.content.strip()


# ----------------------- NLP PROCESSING -----------------------

def correct_hindi_conversation(raw_conversation: str) -> str:
    chunks = group_chunks_by_size(split_by_speaker_sentences(raw_conversation))
#    return "\n\n".join(request_openai(hindi_correction_prompt(raw_conversation)) for chunk in chunks)
    return "\n\n".join(request_openai(hindi_correction_prompt(chunk)) for chunk in chunks)


def translate_hindi_to_english(hindi_text: str) -> str:
    chunks = group_chunks_by_size(split_by_speaker_sentences(hindi_text))
#    return "\n".join(request_openai(translation_prompt(hindi_text), model="gpt-4o") for chunk in chunks)
    return "\n".join(request_openai(translation_prompt(chunk), model="gpt-4o") for chunk in chunks)


def analyze_sales_call(transcript: str) -> str:
    content = request_openai(sales_call_analysis_prompt(transcript), model="gpt-4o")
    return content[8:-4]  # clean unwanted prefix/suffix


def format_transcription(words: list) -> str:
    """Formats word-level transcription into speaker-tagged lines."""
    output, current_line = [], []
    speaker_map, speaker_count = {}, 1
    current_speaker = None

    for word in words:
        if getattr(word, "type", None) not in ["word", "spacing"]:
            continue

        speaker_id = getattr(word, "speaker_id", None)
        text = getattr(word, "text", "")

        if speaker_id not in speaker_map:
            speaker_map[speaker_id] = f"Speaker {speaker_count}"
            speaker_count += 1

        if speaker_id == current_speaker:
            current_line.append(text)
        else:
            if current_line:
                output.append(f"{speaker_map[current_speaker]}: {''.join(current_line).strip()}")
            current_line, current_speaker = [text], speaker_id

    if current_line:
        output.append(f"{speaker_map[current_speaker]}: {''.join(current_line).strip()}")

    return '\n'.join(output)


def generate_combined_output(raw_convo: str) -> dict:
    """Runs correction, translation, and analysis on raw transcript."""

    #print("11labs output")
    #print(type(raw_convo), raw_convo)

    #print("✅ Correcting transcription...")
    corrected = correct_hindi_conversation(raw_convo)
    #print("corrected convo")
    #print(type(corrected), corrected)

    #print("🌐 Translating to English...")
    translated = translate_hindi_to_english(corrected)
    #print("translated")
    #print(type(translated), translated)

    #print("🧠 Analyzing the call...")
    analysis = analyze_sales_call(translated)
    #print("analysis")
    #print(type(analysis), analysis)

    return_val = {
        "transcription": translated,
        "analysis": analysis
    }

    print(type(return_val), return_val)
    
    return return_val

# ----------------------- FLASK ROUTES -----------------------

# In-memory job store (use a database or Redis in production)
JOBS = {}

@app.route('/transcribe', methods=['POST'])
def transcribe_audio():
    """Receives audio file and starts background transcription job."""
    if 'audio' not in request.files:
        return jsonify({'error': 'No audio file provided'}), 400

    audio_file = request.files['audio']
    job_id = str(uuid.uuid4())
    audio_path = f'/tmp/{job_id}.wav'
    audio_file.save(audio_path)

    # Optional webhook callback
    webhook_url = request.form.get('webhook_url')

    JOBS[job_id] = {
        'status': 'queued',
        'webhook_url': webhook_url
    }

    # Start processing in a separate thread
    threading.Thread(target=process_audio_job, args=(job_id, audio_path)).start()

    return jsonify({'job_id': job_id, 'status': 'queued'}), 202


def process_audio_job(job_id, audio_path):
    """Performs transcription and processing in the background."""
    try:
        print(f"[{job_id}] 🎙️ Starting transcription...")

        with open(audio_path, 'rb') as audio_data:
            transcription = elevenlabs_client.speech_to_text.convert(
                file=audio_data,
                model_id="scribe_v1",
                tag_audio_events=True,
                num_speakers=2,
                timestamps_granularity="word",
                diarize=True
            )

        print(f"[{job_id}] 📝 Formatting transcription...")
        formatted = format_transcription(transcription.words)
        result = generate_combined_output(formatted)

        JOBS[job_id]['status'] = 'complete'
        JOBS[job_id]['result'] = result

        print(f"[{job_id}] ✅ Job completed successfully.")

        # Webhook callback if provided
        webhook_url = JOBS[job_id].get('webhook_url')
        if webhook_url:
            try:
                print(f"[{job_id}] 📡 Sending callback to webhook...")
                requests.post(webhook_url, json={
                    'job_id': job_id,
                    'status': 'complete',
                    'result': result
                })
            except Exception as e:
                print(f"[{job_id}] ❌ Webhook failed: {e}")

    except Exception as e:
        print(f"[{job_id}] ❌ Error: {str(e)}")
        JOBS[job_id]['status'] = 'failed'
        JOBS[job_id]['error'] = str(e)

    finally:
        if os.path.exists(audio_path):
            os.remove(audio_path)


@app.route('/status/<job_id>', methods=['GET'])
def check_status(job_id):
    """Check the status of a transcription job."""
    job = JOBS.get(job_id)
    if not job:
        return jsonify({'error': 'Job not found'}), 404

    if job['status'] == 'complete':
        print("being returned from the server")
        print(jsonify(job['result']))
        return jsonify(job['result']), 200
    else:
        return jsonify(job), 202


# ----------------------- MAIN ENTRY -----------------------

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=8001)
